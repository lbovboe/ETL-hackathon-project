{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5348cd9b-0bab-4d52-988b-f09ea0c010de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 12:23:34,986 - __main__ - INFO - Initialized ParquetDataChecker for: ../data/daily_spending_sample.parquet\n",
      "2025-10-18 12:23:34,989 - __main__ - INFO - Reading parquet file metadata...\n",
      "2025-10-18 12:23:34,992 - __main__ - INFO - File contains 6,000 rows and 7 columns\n",
      "2025-10-18 12:23:34,993 - __main__ - INFO - File size: 0.09 MB\n",
      "2025-10-18 12:23:34,996 - __main__ - INFO - Reading first 500 rows...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports completed successfully\n",
      "✓ ParquetDataChecker class defined successfully\n",
      "Using parquet file: ../data/daily_spending_sample.parquet\n",
      "✓ Checker initialized successfully\n",
      "================================================================================\n",
      "FILE METADATA\n",
      "================================================================================\n",
      "File Path: ../data/daily_spending_sample.parquet\n",
      "File Size: 0.09 MB\n",
      "Total Rows: 6,000\n",
      "Total Columns: 7\n",
      "Row Groups: 1\n",
      "\n",
      "Column Names:\n",
      "  1. person_name\n",
      "  2. spending_date\n",
      "  3. category\n",
      "  4. amount\n",
      "  5. location\n",
      "  6. description\n",
      "  7. payment_method\n",
      "\n",
      "Schema:\n",
      "<pyarrow._parquet.ParquetSchema object at 0x1063a3840>\n",
      "required group field_id=-1 schema {\n",
      "  optional binary field_id=-1 person_name (String);\n",
      "  optional binary field_id=-1 spending_date (String);\n",
      "  optional binary field_id=-1 category (String);\n",
      "  optional binary field_id=-1 amount (String);\n",
      "  optional binary field_id=-1 location (String);\n",
      "  optional binary field_id=-1 description (String);\n",
      "  optional binary field_id=-1 payment_method (String);\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SAMPLE DATA (First 500 rows)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_name</th>\n",
       "      <th>spending_date</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>payment_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>01-Apr-2022</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>155.66</td>\n",
       "      <td>Shopee Mart</td>\n",
       "      <td>Household items</td>\n",
       "      <td>Debit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>01-Apr-2023</td>\n",
       "      <td>Transport</td>\n",
       "      <td>$40.10</td>\n",
       "      <td>MRT Station</td>\n",
       "      <td>Shopping trip</td>\n",
       "      <td>Mobile Payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>01-Apr-2023</td>\n",
       "      <td>Shopping</td>\n",
       "      <td>333.95 SGD</td>\n",
       "      <td>Zalora</td>\n",
       "      <td>Online shopping</td>\n",
       "      <td>Apple Pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>01-Apr-2023</td>\n",
       "      <td>Food</td>\n",
       "      <td>SGD 17.51</td>\n",
       "      <td>Restaurant ABC</td>\n",
       "      <td>Dinner</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>01-Aug-2023</td>\n",
       "      <td>Transport</td>\n",
       "      <td>SGD 26.22</td>\n",
       "      <td>Bus Interchange</td>\n",
       "      <td>Taxi to office</td>\n",
       "      <td>EZ-Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>11/01/2024</td>\n",
       "      <td>Education</td>\n",
       "      <td>$105.43</td>\n",
       "      <td>Library Fine</td>\n",
       "      <td>Training</td>\n",
       "      <td>Bank Transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>11/01/23</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>176.34</td>\n",
       "      <td>Sheng Siong</td>\n",
       "      <td>Household items</td>\n",
       "      <td>Mobile Payment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>11/02/2022</td>\n",
       "      <td>Groceries</td>\n",
       "      <td>SGD 126.59</td>\n",
       "      <td>NTUC FairPrice</td>\n",
       "      <td>Weekly Groceries</td>\n",
       "      <td>PayNow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>11/02/2023</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>54.86</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>Movie Tickets</td>\n",
       "      <td>Google Pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>David Wong</td>\n",
       "      <td>11/02/2023</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>$103.71</td>\n",
       "      <td>Spotify</td>\n",
       "      <td>Gaming</td>\n",
       "      <td>Debit Card</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_name spending_date       category      amount         location  \\\n",
       "0    David Wong   01-Apr-2022      Groceries      155.66      Shopee Mart   \n",
       "1    David Wong   01-Apr-2023      Transport      $40.10      MRT Station   \n",
       "2    David Wong   01-Apr-2023       Shopping  333.95 SGD           Zalora   \n",
       "3    David Wong   01-Apr-2023           Food   SGD 17.51   Restaurant ABC   \n",
       "4    David Wong   01-Aug-2023      Transport   SGD 26.22  Bus Interchange   \n",
       "..          ...           ...            ...         ...              ...   \n",
       "495  David Wong    11/01/2024      Education     $105.43     Library Fine   \n",
       "496  David Wong      11/01/23      Groceries      176.34      Sheng Siong   \n",
       "497  David Wong    11/02/2022      Groceries  SGD 126.59   NTUC FairPrice   \n",
       "498  David Wong    11/02/2023  Entertainment       54.86          Spotify   \n",
       "499  David Wong    11/02/2023  Entertainment     $103.71          Spotify   \n",
       "\n",
       "          description  payment_method  \n",
       "0     Household items      Debit Card  \n",
       "1       Shopping trip  Mobile Payment  \n",
       "2     Online shopping       Apple Pay  \n",
       "3              Dinner     Credit Card  \n",
       "4      Taxi to office         EZ-Link  \n",
       "..                ...             ...  \n",
       "495          Training   Bank Transfer  \n",
       "496   Household items  Mobile Payment  \n",
       "497  Weekly Groceries          PayNow  \n",
       "498     Movie Tickets      Google Pay  \n",
       "499            Gaming      Debit Card  \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 12:23:35,021 - __main__ - INFO - Starting data quality validation...\n",
      "2025-10-18 12:23:35,022 - __main__ - INFO - Reading parquet file in chunks of 10,000 rows...\n",
      "2025-10-18 12:23:35,036 - __main__ - INFO - Data quality validation complete. Total rows processed: 6,000\n",
      "2025-10-18 12:23:35,037 - __main__ - INFO - Performing data integrity checks...\n",
      "2025-10-18 12:23:35,039 - __main__ - INFO - Reading parquet file in chunks of 10,000 rows...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA QUALITY REPORT\n",
      "================================================================================\n",
      "Total Rows: 6,000\n",
      "Total Columns: 7\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COLUMN INFORMATION\n",
      "--------------------------------------------------------------------------------\n",
      "  person_name                    | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  spending_date                  | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  category                       | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  amount                         | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  location                       | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  description                    | Type: object          | Nulls:        0 ( 0.00%)\n",
      "  payment_method                 | Type: object          | Nulls:        0 ( 0.00%)\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-18 12:23:35,282 - __main__ - INFO - Data integrity checks complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATA INTEGRITY CHECKS\n",
      "================================================================================\n",
      "Negative amounts found: 0\n",
      "Duplicate rows found: 0\n",
      "Invalid dates found: 0\n",
      "================================================================================\n",
      "\n",
      "✓ Data check completed successfully!\n",
      "\n",
      "Column Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Type</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Null %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>person_name</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spending_date</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>category</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amount</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>location</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>description</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>payment_method</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Column    Type  Null Count Null %\n",
       "0     person_name  object           0  0.00%\n",
       "1   spending_date  object           0  0.00%\n",
       "2        category  object           0  0.00%\n",
       "3          amount  object           0  0.00%\n",
       "4        location  object           0  0.00%\n",
       "5     description  object           0  0.00%\n",
       "6  payment_method  object           0  0.00%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ETL Pipeline - Sample Data Checker (Jupyter Notebook Version)\n",
    "# ================================================================\n",
    "# This notebook reads and validates daily spending sample data from parquet files\n",
    "# using chunking techniques for memory-efficient processing.\n",
    "\n",
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "from decimal import Decimal\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Configure logging for Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports completed successfully\")\n",
    "\n",
    "# Cell 2: ParquetDataChecker Class Definition\n",
    "class ParquetDataChecker:\n",
    "    \"\"\"\n",
    "    A class to check and validate parquet files using chunking techniques.\n",
    "    \n",
    "    Attributes:\n",
    "        file_path (Path): Path to the parquet file\n",
    "        chunk_size (int): Number of rows to process per chunk\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str, chunk_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize the ParquetDataChecker.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the parquet file\n",
    "            chunk_size (int): Number of rows to process per chunk (default: 10000)\n",
    "        \"\"\"\n",
    "        self.file_path = Path(file_path)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.parquet_file = None\n",
    "        \n",
    "        if not self.file_path.exists():\n",
    "            raise FileNotFoundError(f\"Parquet file not found: {self.file_path}\")\n",
    "        \n",
    "        logger.info(f\"Initialized ParquetDataChecker for: {self.file_path}\")\n",
    "    \n",
    "    def get_file_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get metadata information about the parquet file.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing file metadata\n",
    "        \"\"\"\n",
    "        logger.info(\"Reading parquet file metadata...\")\n",
    "        \n",
    "        parquet_file = pq.ParquetFile(self.file_path)\n",
    "        \n",
    "        metadata = {\n",
    "            'file_path': str(self.file_path),\n",
    "            'file_size_mb': self.file_path.stat().st_size / (1024 * 1024),\n",
    "            'num_row_groups': parquet_file.num_row_groups,\n",
    "            'total_rows': parquet_file.metadata.num_rows,\n",
    "            'num_columns': parquet_file.metadata.num_columns,\n",
    "            'schema': parquet_file.schema,\n",
    "            'column_names': parquet_file.schema.names,\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"File contains {metadata['total_rows']:,} rows and {metadata['num_columns']} columns\")\n",
    "        logger.info(f\"File size: {metadata['file_size_mb']:.2f} MB\")\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def read_in_chunks(self, columns: Optional[list] = None):\n",
    "        \"\"\"\n",
    "        Generator to read parquet file in chunks using pyarrow.\n",
    "        \n",
    "        Args:\n",
    "            columns (list, optional): List of columns to read. If None, reads all columns.\n",
    "            \n",
    "        Yields:\n",
    "            pd.DataFrame: Chunk of data\n",
    "        \"\"\"\n",
    "        logger.info(f\"Reading parquet file in chunks of {self.chunk_size:,} rows...\")\n",
    "        \n",
    "        parquet_file = pq.ParquetFile(self.file_path)\n",
    "        \n",
    "        for batch in parquet_file.iter_batches(batch_size=self.chunk_size, columns=columns):\n",
    "            df_chunk = batch.to_pandas()\n",
    "            yield df_chunk\n",
    "    \n",
    "    def validate_data_quality(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform data quality checks on the parquet file using chunking.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing data quality metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Starting data quality validation...\")\n",
    "        \n",
    "        # Initialize counters\n",
    "        total_rows = 0\n",
    "        null_counts = {}\n",
    "        column_types = {}\n",
    "        numeric_stats = {}\n",
    "        \n",
    "        # Process data in chunks\n",
    "        for chunk_num, df_chunk in enumerate(self.read_in_chunks(), start=1):\n",
    "            total_rows += len(df_chunk)\n",
    "            \n",
    "            # Track null values\n",
    "            for column in df_chunk.columns:\n",
    "                if column not in null_counts:\n",
    "                    null_counts[column] = 0\n",
    "                    column_types[column] = str(df_chunk[column].dtype)\n",
    "                \n",
    "                null_counts[column] += df_chunk[column].isnull().sum()\n",
    "                \n",
    "                # Calculate numeric statistics\n",
    "                if pd.api.types.is_numeric_dtype(df_chunk[column]):\n",
    "                    if column not in numeric_stats:\n",
    "                        numeric_stats[column] = {\n",
    "                            'min': float('inf'),\n",
    "                            'max': float('-inf'),\n",
    "                            'sum': 0,\n",
    "                            'count': 0\n",
    "                        }\n",
    "                    \n",
    "                    valid_values = df_chunk[column].dropna()\n",
    "                    if len(valid_values) > 0:\n",
    "                        numeric_stats[column]['min'] = min(\n",
    "                            numeric_stats[column]['min'], \n",
    "                            valid_values.min()\n",
    "                        )\n",
    "                        numeric_stats[column]['max'] = max(\n",
    "                            numeric_stats[column]['max'], \n",
    "                            valid_values.max()\n",
    "                        )\n",
    "                        numeric_stats[column]['sum'] += valid_values.sum()\n",
    "                        numeric_stats[column]['count'] += len(valid_values)\n",
    "            \n",
    "            if chunk_num % 10 == 0:\n",
    "                logger.info(f\"Processed {chunk_num} chunks ({total_rows:,} rows)...\")\n",
    "        \n",
    "        # Calculate final statistics\n",
    "        null_percentages = {\n",
    "            col: (count / total_rows * 100) if total_rows > 0 else 0\n",
    "            for col, count in null_counts.items()\n",
    "        }\n",
    "        \n",
    "        for column in numeric_stats:\n",
    "            if numeric_stats[column]['count'] > 0:\n",
    "                numeric_stats[column]['mean'] = (\n",
    "                    numeric_stats[column]['sum'] / numeric_stats[column]['count']\n",
    "                )\n",
    "        \n",
    "        quality_report = {\n",
    "            'total_rows': total_rows,\n",
    "            'total_columns': len(column_types),\n",
    "            'column_types': column_types,\n",
    "            'null_counts': null_counts,\n",
    "            'null_percentages': null_percentages,\n",
    "            'numeric_statistics': numeric_stats\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Data quality validation complete. Total rows processed: {total_rows:,}\")\n",
    "        \n",
    "        return quality_report\n",
    "    \n",
    "    def display_sample_data(self, n_rows: int = 10):\n",
    "        \"\"\"\n",
    "        Display sample rows from the parquet file.\n",
    "        \n",
    "        Args:\n",
    "            n_rows (int): Number of rows to display (default: 10)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Reading first {n_rows} rows...\")\n",
    "        \n",
    "        parquet_file = pq.ParquetFile(self.file_path)\n",
    "        \n",
    "        # Read only the first batch\n",
    "        first_batch = next(parquet_file.iter_batches(batch_size=n_rows))\n",
    "        df_sample = first_batch.to_pandas()\n",
    "        \n",
    "        print(f\"\\nSAMPLE DATA (First {len(df_sample)} rows)\")\n",
    "        print(\"=\"*80)\n",
    "        display(df_sample)\n",
    "        \n",
    "        return df_sample\n",
    "    \n",
    "    def print_quality_report(self, quality_report: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Print a formatted data quality report.\n",
    "        \n",
    "        Args:\n",
    "            quality_report (Dict): Quality report dictionary\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DATA QUALITY REPORT\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"Total Rows: {quality_report['total_rows']:,}\")\n",
    "        print(f\"Total Columns: {quality_report['total_columns']}\")\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"COLUMN INFORMATION\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for col, dtype in quality_report['column_types'].items():\n",
    "            null_count = quality_report['null_counts'][col]\n",
    "            null_pct = quality_report['null_percentages'][col]\n",
    "            print(f\"  {col:30s} | Type: {dtype:15s} | Nulls: {null_count:8,} ({null_pct:5.2f}%)\")\n",
    "        \n",
    "        if quality_report['numeric_statistics']:\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(\"NUMERIC COLUMN STATISTICS\")\n",
    "            print(\"-\"*80)\n",
    "            \n",
    "            for col, stats in quality_report['numeric_statistics'].items():\n",
    "                print(f\"\\n  {col}:\")\n",
    "                print(f\"    Min:   {stats['min']:,.2f}\")\n",
    "                print(f\"    Max:   {stats['max']:,.2f}\")\n",
    "                print(f\"    Mean:  {stats['mean']:,.2f}\")\n",
    "                print(f\"    Count: {stats['count']:,}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    def check_data_integrity(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform specific data integrity checks for financial data.\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing integrity check results\n",
    "        \"\"\"\n",
    "        logger.info(\"Performing data integrity checks...\")\n",
    "        \n",
    "        integrity_issues = {\n",
    "            'negative_amounts': 0,\n",
    "            'duplicate_rows': 0,\n",
    "            'invalid_dates': 0,\n",
    "            'outliers': []\n",
    "        }\n",
    "        \n",
    "        seen_rows = set()\n",
    "        \n",
    "        for df_chunk in self.read_in_chunks():\n",
    "            # Check for negative amounts (if amount columns exist)\n",
    "            amount_columns = [col for col in df_chunk.columns if 'amount' in col.lower()]\n",
    "            for col in amount_columns:\n",
    "                if pd.api.types.is_numeric_dtype(df_chunk[col]):\n",
    "                    negative_count = (df_chunk[col] < 0).sum()\n",
    "                    integrity_issues['negative_amounts'] += negative_count\n",
    "            \n",
    "            # Check for duplicates (using hash of row values)\n",
    "            for idx, row in df_chunk.iterrows():\n",
    "                row_hash = hash(tuple(row))\n",
    "                if row_hash in seen_rows:\n",
    "                    integrity_issues['duplicate_rows'] += 1\n",
    "                else:\n",
    "                    seen_rows.add(row_hash)\n",
    "            \n",
    "            # Check for invalid dates\n",
    "            date_columns = [col for col in df_chunk.columns if 'date' in col.lower()]\n",
    "            for col in date_columns:\n",
    "                if pd.api.types.is_datetime64_any_dtype(df_chunk[col]):\n",
    "                    invalid_dates = df_chunk[col].isnull().sum()\n",
    "                    integrity_issues['invalid_dates'] += invalid_dates\n",
    "        \n",
    "        logger.info(\"Data integrity checks complete.\")\n",
    "        \n",
    "        return integrity_issues\n",
    "\n",
    "print(\"✓ ParquetDataChecker class defined successfully\")\n",
    "\n",
    "# Cell 3: Define File Path (UPDATE THIS PATH!)\n",
    "# UPDATE THIS PATH to point to your parquet file\n",
    "parquet_file = \"../data/daily_spending_sample.parquet\"\n",
    "\n",
    "# Alternative ways to specify the path:\n",
    "# parquet_file = Path.cwd() / \"data\" / \"daily_spending_sample.parquet\"\n",
    "# parquet_file = \"/full/path/to/your/file.parquet\"\n",
    "\n",
    "print(f\"Using parquet file: {parquet_file}\")\n",
    "\n",
    "# Cell 4: Initialize Checker\n",
    "try:\n",
    "    checker = ParquetDataChecker(str(parquet_file), chunk_size=10000)\n",
    "    print(\"✓ Checker initialized successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please update the parquet_file path in Cell 3\")\n",
    "\n",
    "# Cell 5: Display File Metadata\n",
    "print(\"=\"*80)\n",
    "print(\"FILE METADATA\")\n",
    "print(\"=\"*80)\n",
    "metadata = checker.get_file_metadata()\n",
    "print(f\"File Path: {metadata['file_path']}\")\n",
    "print(f\"File Size: {metadata['file_size_mb']:.2f} MB\")\n",
    "print(f\"Total Rows: {metadata['total_rows']:,}\")\n",
    "print(f\"Total Columns: {metadata['num_columns']}\")\n",
    "print(f\"Row Groups: {metadata['num_row_groups']}\")\n",
    "print(\"\\nColumn Names:\")\n",
    "for i, col in enumerate(metadata['column_names'], start=1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "print(\"\\nSchema:\")\n",
    "print(metadata['schema'])\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Cell 6: Display Sample Data\n",
    "sample_df = checker.display_sample_data(n_rows=500)\n",
    "\n",
    "# Cell 7: Perform Data Quality Validation\n",
    "quality_report = checker.validate_data_quality()\n",
    "checker.print_quality_report(quality_report)\n",
    "\n",
    "# Cell 8: Perform Data Integrity Checks\n",
    "integrity_issues = checker.check_data_integrity()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA INTEGRITY CHECKS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Negative amounts found: {integrity_issues['negative_amounts']:,}\")\n",
    "print(f\"Duplicate rows found: {integrity_issues['duplicate_rows']:,}\")\n",
    "print(f\"Invalid dates found: {integrity_issues['invalid_dates']:,}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(\"✓ Data check completed successfully!\")\n",
    "\n",
    "# Cell 9: Optional - Create Summary DataFrame\n",
    "# Create a nice summary dataframe of column information\n",
    "summary_data = []\n",
    "for col in quality_report['column_types'].keys():\n",
    "    summary_data.append({\n",
    "        'Column': col,\n",
    "        'Type': quality_report['column_types'][col],\n",
    "        'Null Count': quality_report['null_counts'][col],\n",
    "        'Null %': f\"{quality_report['null_percentages'][col]:.2f}%\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nColumn Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Cell 10: Optional - Visualize Numeric Statistics (if you have matplotlib)\n",
    "# Uncomment if you want to create visualizations\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# if quality_report['numeric_statistics']:\n",
    "#     fig, axes = plt.subplots(1, len(quality_report['numeric_statistics']), \n",
    "#                              figsize=(15, 4))\n",
    "#     if len(quality_report['numeric_statistics']) == 1:\n",
    "#         axes = [axes]\n",
    "#     \n",
    "#     for ax, (col, stats) in zip(axes, quality_report['numeric_statistics'].items()):\n",
    "#         ax.bar(['Min', 'Mean', 'Max'], \n",
    "#                [stats['min'], stats['mean'], stats['max']])\n",
    "#         ax.set_title(col)\n",
    "#         ax.set_ylabel('Value')\n",
    "#         plt.xticks(rotation=45)\n",
    "#     \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab46ea0-17b5-4257-8b64-bffd6e93447c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
