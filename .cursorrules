# ETL Hackathon Project - Financial Data Pipeline

## Project Overview
This project builds an intelligent ETL (Extract, Transform, Load) pipeline for financial data analysis. The system transforms scattered, multi-year financial records from various sources (card payments, bank transfers, e-commerce transactions) into structured, analyzable datasets.

## Project Goals
- Automate collection, cleaning, and transformation of financial data
- Enable analysis of spending trends and customer behavior
- Support evaluation of financial strategies and marketing policies
- Provide actionable insights for banks, fintech companies, and digital commerce platforms (Shopee, Grab, PayNow, etc.)
- Bridge the gap between raw financial data and actionable intelligence
- **ULTIMATE GOAL**: Answer "What lifestyle choices can improve my financial burden?"

## Target Use Cases
- Banks and financial institutions
- Fintech companies
- E-commerce platforms
- Digital payment services
- Customer engagement and personalization
- Financial planning and forecasting

## Technical Architecture
The project follows a 5-stage data warehouse architecture:

```
┌─────────────────────────────────────────────────────────┐
│              ETL PIPELINE ARCHITECTURE                   │
└─────────────────────────────────────────────────────────┘

STAGE 1: SRC (Source/Raw Layer) ✅ COMPLETE
  Purpose: Load raw data as-is from files
  Tables: src_daily_spending, src_monthly_income
  Features: Batch tracking, error logging, atomic transactions
  ↓

STAGE 2: STG (Staging/Normalized Layer) ✅ COMPLETE
  Purpose: Clean, normalize to 3NF star schema
  Tables: 4 dimensions + 1 fact table + 1 view
  Features: Data quality scoring, business rules, lineage tracking
  ↓

STAGE 3: CURATED (Snapshot Layer) ✅ COMPLETE
  Purpose: Versioned snapshots of ALL historical data for time-travel analysis
  Tables: curated_spending_snapshots
  Features: Full data snapshots per version, is_latest flag, point-in-time queries
  ↓

STAGE 4: DST (Dissemination Staging/Aggregation) ❌ NOT STARTED
  Purpose: Pre-aggregated metrics for fast reporting
  Tables: Monthly summaries, category trends, person analytics
  Features: Pre-calculated KPIs, optimized for dashboards
  ↓

STAGE 5: DIS (Dissemination Views/Insights) ❌ NOT STARTED
  Purpose: Business insights & financial recommendations
  Views: Financial recommendations, spending patterns, budget alerts
  Features: Actionable insights, lifestyle recommendations
```

**Current Progress: 60% Complete (3 of 5 stages finished)**

## Code Standards & Best Practices

### Data Engineering
- Write robust data validation and error handling
- Implement data quality checks at each pipeline stage
- Use appropriate data types for financial values (avoid floating point precision issues)
- Handle missing data gracefully
- Maintain data lineage and audit trails
- Consider data privacy and security (PII handling, encryption)

### Python Development
- Follow PEP 8 style guidelines
- Use type hints for function parameters and returns
- Write modular, reusable code
- Implement comprehensive logging
- Add docstrings to all functions and classes
- Use pandas for data manipulation
- Consider performance optimization for large datasets

### SQL Development
- Write clear, well-formatted queries
- Use appropriate indexes for performance
- Implement proper data normalization
- Add comments for complex queries
- Use transactions for data integrity

### Testing & Quality
- Write unit tests for transformation logic
- Validate data quality at each stage
- Test edge cases (null values, duplicates, invalid formats)
- Monitor pipeline performance and failures

### Documentation
- Document data schemas and field definitions
- Maintain README with setup instructions
- Document pipeline workflows and dependencies
- Include sample data formats and examples

## Project Structure
```
/ETL-hackathon-project/
├── /data/                          # Raw and processed data files
│   └── daily_spending_sample.parquet
├── /notebooks/                     # Jupyter notebooks for exploration
│   ├── 01_generate_sample_data.ipynb
│   ├── 02_check_sample_data.ipynb
│   ├── 03_testing_supabase_env.ipynb
│   └── 04_test_database_connection.ipynb
├── /scripts/                       # Python ETL pipeline scripts
│   ├── /00_logging_stage/         # ✅ Logging infrastructure
│   │   └── logging_tables_creation.py
│   ├── /01_src_stage/             # ✅ COMPLETE - Raw data layer
│   │   ├── 01_src_spending_creation.py
│   │   ├── 02_load_parquet_to_src.py
│   │   └── 03_run_validation.py
│   ├── /02_stg_stage/             # ✅ COMPLETE - Normalized layer
│   │   ├── 01_stg_tables_creation.py
│   │   ├── 02_transform_and_load_stg.py
│   │   ├── 03_data_quality_report.py
│   │   ├── README.md
│   │   └── IMPLEMENTATION_SUMMARY.md
│   ├── /03_curated_stage/         # ✅ COMPLETE - Versioned snapshots
│   │   ├── 01_cur_tables_creation.py
│   │   ├── 02_create_snapshot.py
│   │   ├── 03_validation_report.py
│   │   └── README.md
│   ├── /04_dst_stage/             # ❌ NOT STARTED - Aggregation layer
│   │   └── [Future work]
│   └── /05_dis_stage/             # ❌ NOT STARTED - Insights layer
│       └── [Future work]
├── /sql/                          # SQL DDL and queries
│   ├── /00_logging_stage/         # ✅ Log tables
│   │   └── log_01_create_table.sql
│   ├── /01_src_stage/             # ✅ Source tables
│   │   ├── src_01_create_tables.sql
│   │   └── src_02_error_validation.sql
│   ├── /02_stg_stage/             # ✅ Staging tables
│   │   └── stg_01_create_tables.sql
│   ├── /03_curated_stage/         # ✅ COMPLETE
│   │   ├── cur_01_create_table.sql
│   │   └── cur_02_helper_queries.sql
│   ├── /04_dst_stage/             # ❌ NOT STARTED
│   │   └── [Future work]
│   └── /05_dis_stage/             # ❌ NOT STARTED
│       └── [Future work]
├── requirements.txt               # Python dependencies
└── .cursorrules                   # Project guidelines (this file)
```

## Data Handling Considerations
- Financial data requires high accuracy (use Decimal for currency)
- Timestamps should be timezone-aware
- Handle multiple currencies appropriately
- Maintain transaction integrity
- Consider data retention policies
- Implement proper access controls

## Key Technologies
- Python for data processing
- Pandas for data manipulation
- SQL for data storage and querying
- Jupyter notebooks for analysis and visualization

## Security & Privacy
- Never commit actual financial data or PII
- Use environment variables for sensitive configuration
- Implement data anonymization for non-production environments
- Follow compliance requirements (GDPR, PCI-DSS, etc.)

## Performance Considerations
- Optimize for large datasets (millions of transactions)
- Use chunking for memory-efficient processing
- Implement incremental data loading
- Consider parallel processing for transformations
- Monitor resource usage and bottlenecks

---

## 📊 PROJECT PROGRESS TRACKING

### Overall Status: 60% Complete (3 of 5 stages)

```
Progress Bar:
├── Stage 1 (SRC):     ████████████████████ 100% ✅
├── Stage 2 (STG):     ████████████████████ 100% ✅
├── Stage 3 (CURATED): ████████████████████ 100% ✅
├── Stage 4 (DST):     ░░░░░░░░░░░░░░░░░░░░   0% ❌ ← NEXT
└── Stage 5 (DIS):     ░░░░░░░░░░░░░░░░░░░░   0% ❌

Estimated Time to Completion: 5-7 hours
```

---

## ✅ COMPLETED STAGES

### STAGE 1: SRC (Source Layer) - COMPLETE ✅

**Purpose:** Load raw data as-is from files into the database

**Completed Deliverables:**
- ✅ Raw data tables: `src_daily_spending`, `src_monthly_income`
- ✅ Error logging infrastructure: `etl_logs` table
- ✅ Data loading scripts with chunked processing (memory-efficient)
- ✅ Atomic transaction handling (all-or-nothing loads)
- ✅ Batch tracking system: `load_batch_id`, `source_file`, `loaded_at`
- ✅ Data validation queries
- ✅ Requirements.txt for Python dependencies

**Key Features Implemented:**
- Parquet file reading with pandas
- Chunk-based processing for large files
- PostgreSQL COPY command for fast inserts
- Comprehensive error logging
- Data lineage tracking from source files

**Files Created:**
- `sql/01_src_stage/src_01_create_tables.sql`
- `scripts/01_src_stage/01_src_spending_creation.py`
- `scripts/01_src_stage/02_load_parquet_to_src.py`
- `scripts/01_src_stage/03_run_validation.py`
- `sql/01_src_stage/src_02_error_validation.sql`

---

### STAGE 2: STG (Staging Layer) - COMPLETE ✅

**Purpose:** Clean, normalize, and transform raw data into a 3NF star schema

**Completed Deliverables:**
- ✅ 4 Dimension tables (normalized reference data):
  - `stg_dim_person` - Person/household information
  - `stg_dim_category` - Spending categories
  - `stg_dim_location` - Transaction locations
  - `stg_dim_payment_method` - Payment method types
- ✅ 1 Fact table: `stg_fact_spending` - Transaction facts with foreign keys
- ✅ 1 Analytical view: `vw_stg_spending_enriched` - Denormalized view for easy querying
- ✅ ETL transformation script with advanced features
- ✅ Data quality scoring system (0-100 points)
- ✅ Comprehensive validation and reporting

**Key Features Implemented:**
- Multi-format date parsing (handles various date formats)
- Currency cleaning (removes symbols, handles decimals)
- Business rule classification (auto-categorizes transactions)
- Data quality scoring with weighted metrics:
  - Required fields (30 points)
  - Date validity (20 points)
  - Amount validity (20 points)
  - Description quality (15 points)
  - Category assignment (15 points)
- Idempotent loading (ON CONFLICT DO UPDATE for reruns)
- Full data lineage tracking (src_id references)
- Validation script with quality grading (A+, A, B, C, D, F)

**Files Created:**
- `sql/02_stg_stage/stg_01_create_tables.sql`
- `scripts/02_stg_stage/01_stg_tables_creation.py`
- `scripts/02_stg_stage/02_transform_and_load_stg.py`
- `scripts/02_stg_stage/03_data_quality_report.py`
- `scripts/02_stg_stage/README.md`
- `scripts/02_stg_stage/IMPLEMENTATION_SUMMARY.md`

**Database Schema:**
```sql
-- Dimension Tables (reference data)
stg_dim_person (person_id, full_name, email, phone)
stg_dim_category (category_id, category_name, parent_category, category_type)
stg_dim_location (location_id, location_name, city, country, region)
stg_dim_payment_method (payment_method_id, payment_method_name, payment_type)

-- Fact Table (transactions)
stg_fact_spending (
  spending_id, date, amount_usd, description,
  person_id FK, category_id FK, location_id FK, payment_method_id FK,
  data_quality_score, src_id FK, processed_at
)

-- Analytical View
vw_stg_spending_enriched (joins all dimensions for easy analysis)
```

---

### STAGE 3: CURATED (Snapshot Layer) - COMPLETE ✅

**Purpose:** Create versioned snapshots of ALL historical spending data for point-in-time analysis

**Completed Deliverables:**
- ✅ 1 Snapshot table: `curated_spending_snapshots` (28 columns, 8 indexes)
- ✅ Versioned snapshot creation script
- ✅ Validation and quality report script
- ✅ 22+ helper queries for common analysis patterns
- ✅ Comprehensive documentation (README.md)

**Key Features Implemented:**
- Full historical snapshot (copies ALL data per version)
- Version management with auto-increment (1, 2, 3...)
- `is_latest` flag (only 1 version has `is_latest = 1` at any time)
- Denormalized dimensions for fast queries without joins
- Point-in-time queries (time-travel to any historical version)
- Growth tracking and version comparison
- Data lineage (links to SRC and STG)
- Comprehensive validation (5 quality checks)

**Files Created:**
- `sql/03_curated_stage/cur_01_create_table.sql`
- `sql/03_curated_stage/cur_02_helper_queries.sql`
- `scripts/03_curated_stage/01_cur_tables_creation.py`
- `scripts/03_curated_stage/02_create_snapshot.py`
- `scripts/03_curated_stage/03_validation_report.py`
- `scripts/03_curated_stage/README.md`

**Database Schema:**
```sql
-- Snapshot Table (28 columns)
curated_spending_snapshots (
  snapshot_id, snapshot_version, snapshot_date, snapshot_batch_id, is_latest,
  src_id, stg_spending_id,
  person_id, category_id, location_id, payment_method_id,
  person_name, category_name, category_group, location_name, location_type,
  payment_method_name, payment_type,
  spending_date, spending_year, spending_month, spending_quarter, spending_day_of_week,
  amount_cleaned, currency_code, description,
  data_quality_score, created_at
)

-- 8 Performance Indexes including partial index on is_latest
```

**How It Works:**
```
Oct 20: Run script → Version 1 with 1000 records (is_latest=1)
Oct 21: Run script → Version 1 (is_latest=0), Version 2 with 1050 records (is_latest=1)
Oct 22: Run script → V1&V2 (is_latest=0), Version 3 with 1080 records (is_latest=1)
```

---

## 🎯 CURRENT STAGE: STAGE 4 - DST (Next To Build)

**Purpose:** Create pre-aggregated tables optimized for fast reporting and analytics

---

## ❌ FUTURE STAGES (Not Yet Started)

### STAGE 4: DST (Dissemination Staging) - NOT STARTED ❌

**Purpose:** Create pre-aggregated tables optimized for fast reporting and analytics

**Planned Deliverables:**
- [ ] `dst_monthly_spending_summary` - Monthly totals by person, category, location
- [ ] `dst_category_trends` - Category spending over time with MoM/YoY changes
- [ ] `dst_person_analytics` - Per-person spending patterns and statistics
- [ ] `dst_payment_method_summary` - Payment method usage trends
- [ ] ETL scripts to populate aggregation tables
- [ ] Incremental update logic (only process new snapshots)
- [ ] Validation scripts

**Key Features to Implement:**
- Pre-calculated monthly aggregations
- Month-over-month (MoM) trend calculations
- Year-over-year (YoY) comparisons
- Top N categories by spending
- Average daily/weekly/monthly spending per person
- Payment method preference analysis
- Incremental updates (process only new curated snapshots)

**Files to Create:**
- `sql/04_dst_stage/dst_01_create_tables.sql`
- `scripts/04_dst_stage/01_dst_tables_creation.py`
- `scripts/04_dst_stage/02_populate_monthly_summary.py`
- `scripts/04_dst_stage/03_populate_category_trends.py`
- `scripts/04_dst_stage/04_populate_person_analytics.py`
- `scripts/04_dst_stage/05_populate_payment_summary.py`
- `scripts/04_dst_stage/06_run_validation.py`
- `scripts/04_dst_stage/README.md`

**Estimated Time:** 3-4 hours

**Why This Stage Matters:**
- Pre-aggregated data = instant dashboard queries (no need to aggregate on the fly)
- Supports business intelligence tools (Tableau, Power BI, Looker)
- Enables fast trend analysis without scanning millions of transactions

---

### STAGE 5: DIS (Dissemination Views) - NOT STARTED ❌

**Purpose:** Create business-friendly views and analytical queries that answer the key question: "What lifestyle choices can improve my financial burden?"

**Planned Deliverables:**
- [ ] `vw_current_spending_summary` - Latest snapshot overview
- [ ] `vw_monthly_category_breakdown` - Spending by category with percentages
- [ ] `vw_spending_trends` - WoW/MoM trends with trend indicators (↑↓)
- [ ] `vw_top_spending_categories` - Top 10 categories with period comparisons
- [ ] `vw_financial_recommendations` ⭐ **KEY VIEW** - Actionable insights for cost reduction
- [ ] `vw_budget_alerts` - Categories exceeding thresholds, unusual patterns
- [ ] Insight generation queries
- [ ] Dashboard query templates

**Key Features to Implement:**
- Spending pattern analysis (identify high-cost categories)
- Cost reduction recommendations (suggest areas to cut spending)
- Lifestyle improvement suggestions (healthier, cheaper alternatives)
- Budget threshold alerts (overspending warnings)
- Trend-based predictions (projected future spending)
- Comparison to peer benchmarks (optional: compare to similar households)

**Example Recommendations Logic:**
```sql
-- Identify high-spending categories with reduction potential
SELECT 
  category_name,
  monthly_avg_spending,
  CASE 
    WHEN category_name = 'Dining Out' AND monthly_avg_spending > 500 
      THEN 'Cook at home 2-3 more days per week (potential savings: $200/month)'
    WHEN category_name = 'Transportation' AND monthly_avg_spending > 300
      THEN 'Consider public transport or carpooling (potential savings: $100/month)'
    WHEN category_name = 'Subscriptions' AND monthly_avg_spending > 100
      THEN 'Review and cancel unused subscriptions (potential savings: $50/month)'
    -- Add more rules...
  END as recommendation
FROM dst_monthly_spending_summary
WHERE monthly_avg_spending > category_benchmark
ORDER BY potential_savings DESC;
```

**Files to Create:**
- `sql/05_dis_stage/dis_01_create_views.sql`
- `sql/05_dis_stage/dis_02_financial_insights.sql`
- `sql/05_dis_stage/dis_03_recommendation_engine.sql`
- `sql/05_dis_stage/dis_04_dashboard_queries.sql`
- `scripts/05_dis_stage/01_deploy_views.py`
- `scripts/05_dis_stage/02_generate_insights_report.py`
- `scripts/05_dis_stage/README.md`

**Estimated Time:** 2-3 hours

**Why This Stage Matters:**
- **This is the ultimate goal of the entire project**
- Transforms raw financial data into actionable intelligence
- Directly answers: "What can I do differently to save money?"
- Provides personalized recommendations based on actual spending patterns
- Enables data-driven financial decision-making

---

## 🗓️ PROJECT ROADMAP

### Completed Milestones ✅
- [x] **Milestone 1:** Set up project structure and dependencies (Week 1)
- [x] **Milestone 2:** Build SRC layer with raw data loading (Week 1)
- [x] **Milestone 3:** Build STG layer with normalization and data quality (Week 2)

### Current Milestone 🎯
- [x] **Milestone 4:** Build CURATED layer with snapshot functionality (Week 3) ← **COMPLETED**
  - [x] Design snapshot schema
  - [x] Create snapshot table
  - [x] Build snapshot ETL script
  - [x] Test multiple snapshots
  - [x] Add validation
  - [x] Document usage

### Upcoming Milestones ❌
- [ ] **Milestone 5:** Build DST layer with pre-aggregations (Week 3-4) ← **NEXT**
  - [ ] Design aggregation tables
  - [ ] Create monthly summary table
  - [ ] Create category trends table
  - [ ] Create person analytics table
  - [ ] Build population scripts
  - [ ] Add validation

- [ ] **Milestone 6:** Build DIS layer with insights and recommendations (Week 4)
  - [ ] Design analytical views
  - [ ] Create spending summary view
  - [ ] Create trends view
  - [ ] Create recommendations view ⭐
  - [ ] Build insight generation queries
  - [ ] Test all views

- [ ] **Milestone 7:** Final testing and documentation (Week 4)
  - [ ] End-to-end pipeline testing
  - [ ] Performance optimization
  - [ ] Create main project README
  - [ ] Add usage examples
  - [ ] Create demo dashboard queries

---

## 📋 DETAILED TASK CHECKLIST

### ✅ STAGE 1: SRC - 100% COMPLETE
- [x] Design raw data schema
- [x] Create source tables (src_daily_spending, src_monthly_income)
- [x] Create logging table (etl_logs)
- [x] Build data loading script with chunking
- [x] Implement error handling and logging
- [x] Add batch tracking
- [x] Create validation queries
- [x] Test with sample data

### ✅ STAGE 2: STG - 100% COMPLETE
- [x] Design normalized 3NF star schema
- [x] Create dimension tables (person, category, location, payment_method)
- [x] Create fact table (stg_fact_spending)
- [x] Create enriched analytical view
- [x] Build transformation logic (date parsing, currency cleaning)
- [x] Implement data quality scoring (0-100 scale)
- [x] Add idempotent loading (ON CONFLICT handling)
- [x] Create validation script with quality grading
- [x] Test complete SRC → STG pipeline
- [x] Write documentation (README, IMPLEMENTATION_SUMMARY)

### ✅ STAGE 3: CURATED - 100% COMPLETE
- [x] Design snapshot schema with versioning and is_latest flag
- [x] Create curated_spending_snapshots table with indexes (28 columns, 8 indexes)
- [x] Build snapshot creation ETL script
- [x] Implement version increment logic (MAX + 1)
- [x] Implement is_latest flag management (set old to 0, new to 1)
- [x] Copy ALL historical data from STG for each snapshot
- [x] Create validation queries (version counts, is_latest checks, data accuracy)
- [x] Build validation script with reporting (5 comprehensive checks)
- [x] Create helper queries (22+ analysis queries for version comparison, time-travel)
- [x] Test multiple snapshot runs (verify version increment and is_latest updates)
- [x] Write documentation (README with examples and usage guide)

### ❌ STAGE 4: DST - 0% COMPLETE (FUTURE TASKS)
- [ ] Design aggregation table schemas
- [ ] Create monthly summary table
- [ ] Create category trends table
- [ ] Create person analytics table
- [ ] Create payment method summary table
- [ ] Build monthly summary population script
- [ ] Build category trends population script
- [ ] Build person analytics population script
- [ ] Implement incremental update logic
- [ ] Add validation queries and scripts
- [ ] Test all aggregations
- [ ] Write documentation

### ❌ STAGE 5: DIS - 0% COMPLETE (FUTURE TASKS)
- [ ] Design analytical view schemas
- [ ] Create current spending summary view
- [ ] Create monthly category breakdown view
- [ ] Create spending trends view
- [ ] Create top categories view
- [ ] Create financial recommendations view ⭐
- [ ] Create budget alerts view
- [ ] Build insight generation queries
- [ ] Build recommendation engine logic
- [ ] Create dashboard query templates
- [ ] Test all views with sample data
- [ ] Generate sample insights report
- [ ] Write documentation

---

## 🎯 CURRENT FOCUS: Building DST Layer

**What to build next:**

The DST (Dissemination Staging) layer creates pre-aggregated tables for fast reporting:

1. **Design aggregation schemas** (30 mins):
   - Monthly spending summary table
   - Category trends with MoM/YoY
   - Person-level analytics
   - Payment method summaries

2. **Create aggregation tables** (1 hour):
   - Write `sql/04_dst_stage/dst_01_create_tables.sql`
   - Write `scripts/04_dst_stage/01_dst_tables_creation.py`
   - Execute and verify

3. **Build population scripts** (2-3 hours):
   - Monthly summary ETL
   - Category trends ETL
   - Person analytics ETL
   - Incremental update logic

4. **Add validation** (30 mins):
   - Validation queries and scripts
   - Verify aggregations match source data

5. **Document** (30 mins):
   - Usage guide and query examples

---

## When Assisting with This Project

### General Principles
- Prioritize data accuracy and integrity
- Suggest scalable solutions
- Consider real-world financial data challenges
- Recommend appropriate error handling
- Focus on maintainability and extensibility
- Propose data validation strategies
- Consider edge cases in financial transactions

### Context-Aware Assistance
When helping with this project, always:

1. **Check Current Stage**: The project is at **Stage 4 (DST) - 0% complete**
   - Stages 1 (SRC), 2 (STG), and 3 (CURATED) are fully complete ✅
   - Stage 4 (DST) is the current focus 🎯
   - Stage 5 (DIS) is planned but not started ❌

2. **Understand the Architecture**: This is a 5-stage data warehouse:
   ```
   SRC → STG → CURATED → DST → DIS
   (Raw) (Clean) (Snapshot) (Aggregate) (Insights)
   ```

3. **Follow Established Patterns**: Look at completed stages for coding patterns:
   - Python scripts follow the same structure (connection handling, logging, error handling)
   - SQL scripts are well-commented and modular
   - Each stage has: DDL, ETL scripts, validation scripts, documentation

4. **Maintain Consistency**:
   - Use the same naming conventions (`stg_dim_*`, `stg_fact_*`, etc.)
   - Follow the same file numbering (01_, 02_, 03_)
   - Keep the same code style and patterns
   - Use the same connection handling approach (Supabase with environment variables)

5. **Know the Goal**: The ultimate objective is answering:
   > **"What lifestyle choices can improve my financial burden?"**
   
   All stages build toward this final goal in Stage 5 (DIS).

### Quick Reference: What's Been Built

**✅ SRC Layer** (scripts/01_src_stage/):
- Raw data tables with batch tracking
- Parquet file loading with chunking
- Error logging infrastructure

**✅ STG Layer** (scripts/02_stg_stage/):
- 3NF star schema (4 dimensions + 1 fact)
- Data quality scoring (0-100)
- Multi-format parsing and cleaning
- Idempotent loading (ON CONFLICT)
- Comprehensive validation

**✅ CURATED Layer** (scripts/03_curated_stage/) - COMPLETE:
- Versioned snapshots of ALL historical data (28 columns, 8 indexes)
- Version management with `is_latest` flag
- Time-travel queries (view data as it was on any past date)
- 22+ helper queries for analysis
- Comprehensive validation (5 checks)

**❌ DST Layer** (scripts/04_dst_stage/) - Future ← **NEXT TO BUILD**:
- Pre-aggregated monthly summaries
- Category trends with MoM/YoY
- Person-level analytics

**❌ DIS Layer** (scripts/05_dis_stage/) - Future:
- Financial recommendations ⭐ **KEY DELIVERABLE**
- Spending pattern insights
- Budget alerts and warnings

### Quick Start for Stage 4 (Current Work)

If asked to help with Stage 4 DST, the immediate tasks are:

1. Create directories: `sql/04_dst_stage/` and `scripts/04_dst_stage/`
2. Design aggregation table schemas (monthly summaries, trends, analytics)
3. Write `dst_01_create_tables.sql` with DDL
4. Write `01_dst_tables_creation.py` to execute DDL
5. Write ETL scripts to populate aggregation tables from CURATED
6. Implement incremental update logic
7. Write validation queries and scripts
8. Document everything in README.md

### Database Connection Info
- Database: Supabase (PostgreSQL)
- Connection: Environment variables (SUPABASE_URL, SUPABASE_KEY)
- Look at existing scripts in `scripts/01_src_stage/` or `scripts/02_stg_stage/` for connection examples

### Key Tables to Know

**Source Layer:**
- `src_daily_spending` - Raw spending data
- `etl_logs` - Error and process logging

**Staging Layer:**
- `stg_dim_person`, `stg_dim_category`, `stg_dim_location`, `stg_dim_payment_method` - Dimensions
- `stg_fact_spending` - Fact table
- `vw_stg_spending_enriched` - Denormalized view (source for CURATED snapshots)

**Curated Layer:**
- `curated_spending_snapshots` - Versioned snapshots with is_latest flag (28 columns)

---

## 📊 PROJECT SUMMARY

**Status**: 60% Complete (3 of 5 stages finished)

**Completed**: 
- ✅ Stage 1 (SRC): Raw data loading
- ✅ Stage 2 (STG): Normalized star schema with data quality
- ✅ Stage 3 (CURATED): Versioned snapshots with time-travel

**Current Focus**: 
- 🎯 Stage 4 (DST): Building pre-aggregated tables

**Remaining Work**: 
- ❌ Stage 4 (DST): Pre-aggregated analytics tables (IN PROGRESS)
- ❌ Stage 5 (DIS): Financial insights and recommendations

**Time to Completion**: 5-7 hours of focused development

**Final Goal**: Deliver actionable financial insights answering "What lifestyle choices can improve my financial burden?"

---

**Last Updated**: October 21, 2025
**Current Stage**: DST (Stage 4) - 0% complete
**Next Milestone**: Complete DST layer with pre-aggregated tables

